\documentclass{llncs}

\usepackage{url,amsmath,amssymb}
\usepackage{color}

% Dissemination Plans and Important Dates:

% - Your tutorial shall be acompanied by a paper,
% which will be published as a chapter of a book
% in the "Logic and Foundations of Mathematics" series
% by College Publications.

% - Please send us a good, though not necessarily final,
% version of your paper ** BEFORE 1st OF JUNE **.
% This will give us (and College Publications) sufficient time
% to produce printed copies of a preliminary version of the book,
% to be distributed to registered participants.

% - Afterwards there will be plenty of time to improve
% the paper/chapter and incorporate feedback gained during the event.


% General remarks:

% - The target audience (for both the tutorials and
% the accompanying papers) consists of Ph.D. students,
% young post-docs and researchers from other logic-related communities.

% - Avoid obscurity and clarify concepts that might be
% unknown to people from other communities.

% - Strive for a self-contained paper, but be concise and
% cite papers where readers may find more information.

% - Do not hesitate to build bridges between your domain and
% other proof-related communities if you have some ideas to do so.
% These bridges should lead to interesting discussions during the workshop.

% - You are encouraged to compare what is done in your community
% with what is done in other communities.
% Try to understand and explain why your community does things differently.
% This could lead to interesting discussions too.

% - This template aims at ensuring a
% reasonably uniform style for all speakers.
% Nevertheless, feel free to deviate from the template if you need.
% We are aware that not all questions are applicable to all speakers.


% We hope the questions here will guide you
% in the production of your tutorial.

% If anything is unclear, if you have any questions, contact us!

% Thank you!

\newcommand{\Note}[1]{\textcolor{blue}{[#1]}}

\title{ Proofs in Satisfiability Modulo Theories }
% Please select a general title that reflects
% the community you are going to represent in the event.

\author{
  Clark Barrett \inst{1}
  \and
  Leonardo de Moura \inst{2}
  \and
  Pascal Fontaine \inst{3}
}

\authorrunning{C.\~Barrett \and L.\~de Moura \and P.\~Fontaine}

\institute{
  New York University\\
  \email{barrett@cs.nyu.edu}
  \and
  Microsoft Research \\
  \email{leonardo@microsoft.com}
  \and
  University of Lorraine and INRIA\\
  \email{pascal.fontaine@inria.fr}
}

\begin{document}

\maketitle

\section{Introduction}

Satisfiability Modulo Theories (SMT) solvers\footnote{We refer
  to~\cite{Barrett14} for a survey on SMT.} check the satisfiability of
first-order formulas written in a language containing interpreted predicates
and functions.  These interpreted symbols are defined either by first-order axioms
(e.g.\ the axioms of equality, or array axioms for operators {\tt read} and {\tt write},\dots) or by a
structure (e.g.\ the integer numbers equipped with constants, addition,
equality, and inequalities).  Theories frequently implemented within SMT solvers
include the empty theory (a.k.a.\ the theory of uninterpreted symbols with
equality), linear arithmetic on integers and/or reals, bit-vectors and the
theory of arrays.  A very small example of an input formula for an SMT solver is
\begin{equation}\label{eq:example}
a \leq b \wedge b \leq a + x \wedge x = 0 \wedge
 \big[ f(a) \neq f(b) \vee (q(a) \wedge \neg q(b + x)) \big].
\end{equation}
The above formula uses atoms over a language of equality, linear arithmetic,
and uninterpreted symbols ($q$ and $f$) within some Boolean combination.  The
SMT-LIB language standard (currently in version 2.0~\cite{Barrett15}) is a
standard concrete input language for SMT solvers.  Figure~\ref{fig:smtlib}
presents the above example formula in this format.
%CB: I will address this later
%The format currently does
%not provide any guidelines for proof output.

\begin{figure}
{\footnotesize
\begin{verbatim}
(set-logic QF_UFLRA)
(set-info :source | Example formula in SMT-LIB 2.0 |)
(set-info :smt-lib-version 2.0)
(declare-fun f (Real) Real)
(declare-fun q (Real) Bool)
(declare-fun a () Real)
(declare-fun b () Real)
(declare-fun x () Real)
(assert (and (<= a b) (<= b (+ a x)) (= x 0)
             (or (not (= (f a) (f b))) (and (q a) (not (q (+ b x)))))))
(check-sat)
(exit)
\end{verbatim}
}
\caption{\label{fig:smtlib} Formula~\ref{eq:example}, presented in the SMT-LIB 2.0 language}
\end{figure}

SMT solvers were originally designed as decision procedures for
decidable quantifier-free fragments, but many SMT solvers additionally tackle
quantifiers, and some contain decision procedures for certain decidable quantified fragments
(see e.g.~\cite{Ge1,Ge2}).  For these solvers, refutational completeness for first-order logic
(with equality but without further interpreted symbols) is an explicit goal.
Also, some SMT solvers now deal with theories that are undecidable even in the
quantifier-free case, for instance non-linear arithmetic on integers~\cite{Borralleras1}.

In some aspects, and also in their implementation, SMT solvers can be seen as
extensions of propositional satisfiability (SAT) solvers to more expressive
languages.  They lift the efficiency of SAT solvers to richer logics:
state-of-the-art SMT solvers are able to deal with very large formulas,
containing thousands of atoms.  Very schematically, an SMT solver abstracts its
input to propositional logic by replacing every atom with a fresh proposition,
e.g., for the above example,
\begin{displaymath}
p_{a \leq b} \wedge p_{b \leq a + x} \wedge p_{x = 0} \wedge
 \big[ \neg p_{f(a) = f(b)} \vee (p_{q(a)} \wedge \neg p_{q(b + x)}) \big].
\end{displaymath}
The underlying SAT solver is used to provide Boolean models for this
abstraction, e.g.\
\begin{displaymath}
\{ p_{a \leq b}, p_{b \leq a + x}, p_{x = 0}, \neg p_{f(a) = f(b)} \}
\end{displaymath}
and the theory reasoner repeatedly refutes these models and refines the Boolean
abstraction by adding new clauses (in this case $\neg p_{a \leq b} \vee \neg
p_{b \leq a + x} \vee \neg p_{x = 0} \vee p_{f(a) = f(b)}$) until either the
theory reasoner agrees with the model found by the SAT solver, or the
propositional abstraction is refined to an unsatisfiable formula.  As a
consequence, propositional reasoning and theory reasoning are quite well
distinguished.  Naturally, the interaction between the theory reasoner and the
SAT reasoning is in practice much more subtle than the above naive description,
but even when advanced techniques (e.g.\ online decision procedures and theory propagation, see again~\cite{Barrett14}) are used,
propositional and theory reasoning are not very strongly mixed.  SMT proofs will
also feature an interleaving of SAT proofs and theory reasoning proofs.  The SAT
proofs are mainly some form of resolution proofs, whereas the theory reasoning
results in lemmas, i.e.\ new clauses, that are handled by the SAT solver just
like any other clause in the input.

% PF this should be improved
The theory reasoner may be a decision procedure for one theory, e.g.\ congruence
closure~\cite{Nelson2,Nieuwenhuis6} for uninterpreted symbols or a simplex-based
procedure~\cite{Dutertre1} for linear arithmetics, but it is most of the time a
combination of decision procedures.  Combination
frameworks~\cite{Nelson3,Tinelli1} build decision procedures for sets of
literals mixing interpreted symbols from several decidable languages (like the
aforementioned linear arithmetic and uninterpreted symbols) into one decision
procedure for the union of languages.  Combining decision procedures is possible
if the theories in the combination fulfill some properties.  For instance,
theories used in SMT solvers are often \emph{stably-infinite}\footnote{A theory
  is stably-infinite if every satisfiable set of literals in the theory has an
  infinite model.} and disjoint; this is notably the case for linear arithmetic
on integers and reals, and for uninterpreted symbols.  Combining theories often
either involve guessing which shared terms are equal and which are not, or,
equivalently, communicating disjunctions of equalities between decision
procedures.  Again, proofs for a combination of theories will be built from
proofs related to the theories in the combinations; the component proofs will
simply be combined using Boolean reasoning rules.

Quantifier reasoning in SMT is still mostly done through
instantiation~\cite{Moura9}.  Proofs involving quantifier reasoning are thus
just ground proofs augmented with some instantiation steps.  Skolemization is
however not trivial.  While it is not absolutely clear how to output a good
proof for some preprocessing techniques (e.g.\ Skolemization, and to a lesser
extend clausification) some other preprocessing techniques pose another problem.
Rewriting is pervasive in SMT solving, and it is not easy to be proof producing
for every single rewriting step that can occur in the solver.  More generally,
the main challenge of proof production is to collect and keep enough information
to produce proofs, without hurting efficiently too much.

%\Note{Clark}
% What are the tools that pioneered proof production in your area?
% What are the tools that currently produce proofs?
% How widespread is the feature of proof production
% among tools in your area?
% How was the historical evolution of the
% proof production feature in your area?
% If the tools in your area cannot produce proofs easily, explain why,
% and describe the current trends to alleviate this problem.

The paper is organized as follows.  Section~\ref{sec:impl} discusses the
specifics of what is required to implement proof-production in SMT along with
several challenges.  Next, Section~\ref{sec:history} gives a historical
overview of the work on producing proofs in SMT.  We then discuss (in
Sections~\ref{sec:cvc},~\ref{sec:veriT}, and~\ref{sec:z3}) the particular
approaches taken by CVC4, veriT, and Z3. Section~\ref{sec:lean} includes a
discussion of the new \emph{Lean} prover, which attempts to bridge the gap
between SMT reasoning and proof assistants more directly by building a proof
assistant with efficient and sophisticated built-in SMT capabilities.  Finally,
Section~\ref{sec:app} discusses current applications of SMT proofs and then
Section~\ref{sec:concl} concludes.

%MathSAT5? SMTInterpol?

%% Mention Fx7 - ``Rocket fast proof checking'' paper (TACAS 2008)
%% Proofs and Refutations (Z3) (IWIL 2008)
%% Towards an SMT Proof format SMT 2008, LFML 2008, SMT 2009
%% veriT 2009
%% PxTP 2011

%% History of proofs in SMT.  CVC tools, Z3, veriT
%% MathSAT has support for proofs, but unclear what the format is (brief mention
%% in MathSAT5 paper)

% Is there an open-source and minimalistic
% *proof-producing* version of your kind of tool that would be
% particularly suitable for beginners to look at and modify?
%I fear CVC4 is too large to be good for beginners - how about veriT?


\section{Implementing Proof-Production in SMT}
\label{sec:impl}
%\Note{Pascal}

Since the core of SMT solvers is mainly a SAT solver adapted for the needs of
SMT, the core of the proof is also a SAT proof.  We will refer to the other
article by Marijn Heule, in the same volume, for more details on SAT solving.
Let us just remind that SAT solvers work on a conjunctive normal form, and build
a (partial) assignment using Boolean propagations and decisions.  If building
this assignment fails (i.e.\ it falsifies a clause) because of some decisions,
the algorithm analyses the conflict, learns a new clause that will serve as a
guide to avoid repeating the same wrong decisions again, and backtracks.  Only
this learning phase is important for proofs, and it is easy to generate a
resolution trace corresponding to this conflict analysis and production of the
learned clause.  Proof producing SMT solvers rely on the underlying SAT solver
to produce this resolution proof.

In the context of SAT solving, a conflict is always due to the assignment
falsifying a clause.  Another kind of conflict can occur in SMT: assume the SAT
solver assigned $p_{a=b}$ (i.e.\ the propositional abstraction of the atom
$a=b$) to true, and later $p_{f(a) = f(b)}$ to false.  There may be no clause
conflicting with such an assignment, but it is obviously inconsistent on the
theory level.  The theory reasoner embedded in the SMT solver checks assignments
for theory consistency.  In the present case, it would notify a conflict to the
underlying SAT engine, and would produce the clause $\neg p_{a=b} \vee p_{f(a) =
  f(b)}$.  This theory clause refines the propositional abstraction with some
knowledge from the theory.  This knowledge can then be used like any other
clause.  It theory reasoners provide detailed proofs for theory clauses, the SMT
proof is just the interleaving of these theory proofs and the resolution proofs
generated by the SAT solver.

The satisifiability of sets of ground literals with equalities and uninterpreted
symbols can be checked using congruence
closure~\cite{Nelson2,Nieuwenhuis3,Nieuwenhuis6}.  Efficient algorithms are non
trivial, but the idea of congruence closure algorithms is simple.  It builds a
partition of all relevant terms, according to the equalities, and check for
inconsistencies with negated equalities.  For instance, if the relevant terms
are $a$, $b$, $f(a)$ and $f(b)$, the initial partition would be $\big\{\{a\},
\{b\}, \{f(a)\}, \{f(b)\}\big\}$.  If an equality $a=b$ is asserted, the
algorithm would merge the classes for $a$ and $b$, because of this equality, and
would also merge the classes for $f(a)$ and $f(b)$ because it merged the
arguments of $f$ and because equality is a congruence.  If the algorithm is
aware that $f(a) \neq f(b)$ should hold, it detects a conflict as soon as $f(a)$
and $f(b)$ are put in a some congruence class.  Producing proofs for congruence
closure is thus only a matter of keeping a trace of which (two) terms are merged
and why (i.e.\ either because of a congruence, or an asserted equality).  This
information can be stored in an acyclic undirected\footnote{For efficiency
  reasons, modern congruence closure alogrithms actually use a directed
  graph~\cite{Nieuwenhuis6}.} graph, where terms are nodes, and edges represent
asserted equalities or congruences.  If the equality of two terms can be deduced
by transitivity using asserted equalities and congruences, then there is a
(unique) path between the two terms in the graph.  To prove a congruence
(e.g.\ $g(a,c) = g(b,d)$), it suffices to first prove the equality of the
corresponding arguments ($a=b$, $c=d$) and use an instance of the congruence
axiom schema, e.g.\ $(a=b \wedge c=d) \Rightarrow g(a,c) = g(b,d)$.  Proofs
steps (i.e.\ an equality being the consequence of transitivity, and a congruence
instance) are again simply connected using resolution.

Another very important theory supported by many SMT solvers is linear arithmetic
on reals, and the decision procedures they implement are often based on a
specialized simplex~\cite{Dutertre1}.  The algorithm maintains (a) a set of
linear equalities (b) upper and lower bounds on the variables (c) an
assignement.  The bounds and the linear equalities directly come from the
asserted literals; new variables are introduced so that bounds only apply on
variables and not on more complicated linear combinations.  Starting from an
assignment satisfying all equalities, the procedure tries to progressively
satisfy all bounds on variables.  During the process, the linear equalities are
combined linearly to provide new sets of equalities, always equivalent to the
original set.  If the right strategies are used, it is guaranteed that the
procedure terminates either on an assignment satisfying all equalities and
bounds, or one linear equality will be in contradiction with the bounds on its
variables.  If then suffices to collect the constraints corresponding to the
equality and the bounds to have the assertions involved in the contradiction.
In fact, the coefficients of the variables in the linear equality correspond to
the coefficients of the linear combination related to Farkas'lemma: roughly,
this lemma states that a set of linear inequalities is inconsistent if and only
if there is a linear combination of those inequalities resulting in $1 \leq 0$.

The above procedure only applies to reals.  Various techniques are used to adapt
it for mixed real and integer linear arithmetic.  One is branching: if, in the
real algorithm, a variable $x$ has a rational value (e.g.\ 1.5), the procedure
may issue a new lemma stating $x\leq 1 \vee x \geq 2$.  Notice that this lemma
is a tautology of arithmetic.  Another technique is to introduce cuts; cuts
basically remove from the set of feasible solutions a subset with no integer
solution.  Very schematically, cuts are linear combinations of already available
constraints, strengthened using the fact that variables are integer, e.g.\ $x+y
\leq 1.5$ can be strengthened to $x + y \leq 1$ if $x$ and $y$ are integer
variables.

Among reasoning engines found in modern solvers, one finds non-linear arithmetic
procedures and modules to handle data structures like arrays and bit-vectors.
Their internals may vary a lot from one solver to another, and we will not
consider them in more details here.  For some non-linear arithmetic techniques
(see e.g.~\cite{Bockmayr1}) like cylindrical algebraic decomposition or
virtual substitution, it is not even clear how to produce useful certificates.

Typically, SMT solvers handle quantifiers using instantiation, i.e.\ generating
ground instances of universal formulas.  Lemmas like $\forall \mathbf{x}
\varphi(\mathbf{x}) \Rightarrow \varphi(\mathbf{t})$ --- where $\mathbf{x}$ is a
vector of variables and $\mathbf{t}$ is a vector of terms with the same length
--- are first-order tautologies and can be provided as lemmas.  Since
Skolemization produces new symbols, it does not preserve logical equivalence and
thus requires some care.  Furthermore, for efficiency, it may be useful to resort to
advanced techniques for Skolemization that are even less amenable to proofs.

% Please list the (most common) proof systems
% (e.g. resolution, superposition, tableaux,
% sequent calculus, natural deduction, ...)
% underlying state-of-the-art tools of your kind.

% Please show the inference rules of
% (some of) these proof systems explicitly.
% Use, for example, proof.sty or bussproofs.sty.

% Why are these proof systems particularly useful for
% your automated deduction tools?
% Which features make them suitable?
% What is not so convenient in them?

% What are the trends w.r.t. proof systems for your kind of tool?

%\section{Proof Consumption}

% Does your kind of tool consume proofs from another kind of tool?
% (e.g. SMT-solvers using (proofs from) sat-solvers;
% higher-order ATPs and proof assistants using (proofs from)
% first-order ATPs; ...)

% If so, is there anything that could be improved in the proofs
% that are generated by this other kind of tool?

% If your tool consumes proofs from other tools,
% is it only a proof-checker or is
% it a wider tool also used to build proofs by its own?
% If it is a wider tool, do you also provide a lighter version of
% your tool that is dedicated to proof-checking?
% If not, would it be desirable?

% What are the trends?


\subsection*{Challenges}

%\Note{Clark}
SMT solvers typically include a module for preprocessing formulas to simplify
them before running the main algorithm to search for a satisfying assignment.
This module may perform anything from simple rewrites (such as rewriting $x=y$
to $y=x$) to complex global simplifications that significantly change the
structure of the formula.

Preprocessing poses a challenge for proof-production.  Each transformation done
by the preprocessor must be reflected in the produced proof.

How do the proof search techniques in SMT affect proof production?  For
example, rewriting, CNF conversion, preprocessing are challenging for proofs.
Some processing is typically disabled when producing proofs.

% Which algorithms are used to search for a proof/refutation?
% Is the procedure able to find (counter-)models as well?

% Which criteria (e.g. speed, sizes (or other measures) of proofs/models)
% are used to evaluate whether a procedure is better than another?

% What are the "bottlenecks" that prevent current procedures
% from solving more problems?

% Is there any mismatch between the abstract proof systems and
% the implemented proof search procedures?
% If so, how big is the gap between proof theory
% and automated deduction in your field?
% Would it be possible/desirable to develop proof systems
% that are closer to the actual implementations?

% Are there any particular decision procedures or
% proof search procedures that are not well-covered by proof theory yet?

% Do the proof search procedures guarantee that an optimal proof
% (for some sense of "optimal") will be found, if a proof exists?

% Are there proof search optimization techniques
% (e.g. resolution refinements, subsumption,
% in-processing) that succesfully improve theorem proving efficiency,
% but may be harmful for the generation of "good" (e.g. short, detailed, easy to check, ...) proofs?
% Which techniques are harmful? Which are harmless?

% If the generated proofs are not optimal,
% what kinds of redundancies may they contain?
% Are there methods to improve the proofs in a post-processing phase?
% Would it be possible/profitable to modify the existing
% proof search procedures so that they generate better proofs,
% that do not need to be post-processed afterwards?

% If the generated proofs are optimal but depend on the
% kind of proof-checker that is used to verify the produced proofs,
% explicit the dependency you rely on to generate optimal proofs.

% What are the trends w.r.t. proof search methods for your kind of tool?

\section{A Short History of Proofs in SMT}
\label{sec:history}
The Cooperating Validity Checker (CVC)~\cite{SBD02} was the first SMT solver to
tackle the problem of proof-production.  CVC was built by Aaron Stump and Clark
Barrett, advised by David Dill, at Stanford University.  CVC was designed to
improve upon and replace the Stanford Validity Checker (SVC)~\cite{BDL96} for
use in the group's verification applications and also to serve as a research
platform for studying SMT ideas and algorithms.    CVC
uses a proof format based on the Edinburgh Logical Framework (LF)~\cite{HHP93}
and its proofs can be checked using the flea proof-checker~\cite{SBD02b,SD02}, which was
developed concurrently with CVC.

The motivation for producing proofs was primarliy that it would provide a means
of independently certifying a correct result,
so that users would not have to rely on the correctness of a large and
frequently-changing code base.  Another motivation was the hope that
proof-production could aid in finding and correcting nasty bugs in CVC.
In retrospect, however, the most important and lasting contribution of CVC's
proof infrastructure was that it enabled an efficient integration of a SAT
solver for Boolean reasoning.  Initial attempts to use a SAT solver within
CVC suffered from poor performance because the theory-reasoning part of CVC
was being used as a black box, simply flagging a particular branch of the SAT
search as unsatisfiable.  Without additional information (i.e. a conflict
clause specifying a small reason for the unsatisfiability), the SAT solver could
not benefit from clause learning or non-chronlogical backjumping and frequently
failed to terminate, even on relatively easy problems.  During a conversation
with Cormaq Flanagan, the idea of using the proof infrastructure to compute
conflict clauses emerged, and this was the key to finally making the
integration with SAT efficient (see~\cite{BDS02-CAV02} for more details).  The
technique was implemented in CVC, as well as in Flanagan's solver called
Verifun~\cite{FJO+03} and this laid the groundwork for the so-called DPLL(T)
architecture~\cite{NieOT-JACM-06} used in nearly every modern SMT solver.

The next important development in proof-producing SMT solvers was the
early exploration of using proofs to communicate with skeptical proof
assistants.  The first work in this area~\cite{MBG06} was a translator designed
to import proofs from CVC Lite~\cite{BB04} (the successor to CVC) into HOL
Lite~\cite{H96}, a proof assistant for higher-order logic with a small trusted
core set of rules.  The goals of this work were two-fold: to provide access to
efficient decision procedures within HOL Lite and to enable the use of HOL Lite
as a proof-checker for CVC Lite.  Shortly thereafter~\cite{FMM+06,HCF+07}, a similar effort was
made to integrate the haRVey SMT solver~\cite{DR03} with the Isabelle/HOL prove
assistant~\cite{NPW02}. These early efforts demonstrated the feasibility of such
integrations.

In 2008, an effort was made to leverage this work to certify that the
benchmarks in the SMT-LIB library were correctly labeled (benchmarks are
labeled with a \emph{status} that can be ``sat'', ``unsat'', or ``unknown'',
indicating the expected result when solving the benchmark).  The certification
was done using CVC3~\cite{BT07} (the successor to CVC Lite) and
HOL Lite~\cite{GB08}.  Many of the benchmarks in the library were certified and
additionally, a bug in the library was found as two benchmarks that had been
labeled satisfiable were certified as unsatisfiable.
The same work reports briefly on an anecdote that further validates the value
of proof-production.  A latent bug in CVC3 was revealed during the 2007
SMT-COMP competition.  Using HOL Light as a proof-checker, the cause of the bug
was quickly detected as a faulty proof rule.  The bug in the proof rule had
persisted for years and would have been very difficult to detect without the
aid of the proof-checker.

The same year also marked the appearance of proof-production capabilities in
several additional SMT solvers.  Fx7 was a solver written by Michal Moske\l
which emphasized quantified reasoning and fast proof-checking~\cite{M08}.
MathSAT4 used an internal proof engine to enable the generation of
unsatisfiable cores and interpolants~\cite{BCF+08}.  Z3 began supporting
proof-production as well~\cite{dMB08}, though its use of a single rule for
theory lemmas meant that checking or reconstructing Z3 proofs requires the
external checker to have some automated reasoning capability (see
Section~\ref{sec:z3}).  Finally, development on veriT, the successor to haRVey,
began in earnest~\cite{BdOD+09}, with proof-production as a primary goal (see Section~\ref{sec:veriT}).
Another important community development around this time was an attempt to
converge on a standard proof format for SMT-LIB.  We discuss this effort in
Section~\ref{sec:format}.

At the time of this writing, the state of proofs in SMT are in flux.  A few
groups are making a serious effort to developing solvers (e.g. CVC4 and veriT)
capable of producing self-contained, independently-checkable proofs, though
these are in progress and no standard format has been agreed upon.  Other
solvers (like Z3) produce a trace of deduction steps, but reconstructing a full
proof from these steps requires additional search in some cases.  Still other
solvers (e.g. MathSAT, SMTInterpol), use proof technology primarily to drive
additional solver functionality, like computing explanations, interpolants, or
unsatisfiable cores.  In this paper, our focus is on solvers that can produce
actual proof objects with the goal of having them be checked or translated by
an external tool.

\subsection*{Proof Formats for SMT}
\label{sec:format}

\Note{Clark}
There is no standard format for SMT.  There has been some discussion and even
some proposals.  Mention various proposals.  Below, we will discuss four
implementations of proofs in SMT.  The approaches are different enough to
warrant separate treatment.

Unlike SAT solvers, SMT solvers are usually large pieces of software: besides
\marginpar{PF2All: do not be afraid to remove this if it fits nowhere}
propositional reasoning engines, they include decision procedures that can
themselves be quite complex.  Furthermore, the code of the components (i.e.\ the
various decision procedures and the propositional reasoner) can be intricately
mixed together.  Implementing a new SMT solver from scratch, or even getting
into the code of an existing SMT solver, is a tedious task.  The OpenSMT
solver~\cite{Bruttomesso4} is maybe the one that includes the best in its
philosophy a less stiff learning curve for new developers.


\section{The CVC Family of SMT Solvers}
\label{sec:cvc}

%\Note{Clark}
%\subsection{Proof Formats}
%\subsection{Proof Production}

\section{The veriT SMT Solver}
\label{sec:veriT}

The veriT SMT solver is developed jointly at Loria, Nancy (France) and UFRN,
Natal (Brazil).  It is open-source, under the permissive BSD licence.  veriT is
first a testing plate-form for techniques developed around SMT, but it is
sufficienctly stable to be used by third parties.  Proofs in veriT are mainly
resolution proofs interleaved with theory reasoning lemmas.

The proof trace language of veriT is inspired from the SMT-LIB 2.0 standard.  A
sample proof for our running example~\ref{eq:example} is given in
Figure~\ref{fig:proofverit}.  The language is quite coarse grained and rather
falls into the ``proof trace'' category rather than full detailed proofs.  It
provides however a full account of the resolution proof, and equality reasoning
is broken down into applications of the congruence and transitivity instances of
the axiom schemas for equality.  Symmetry of equality is silently used
uniformly.  Special proof rules are assigned to theory lemmas (from arithmetic),
but veriT does not break down arithmetic reasoning to instances of e.g.,
Presburger axioms.  The example here does not feature quantifier reasoning;
proofs with quantifiers use quantifier instantiation rules.

\begin{figure}
{\scriptsize
\begin{verbatim}
(set .c1 (input :conclusion ((and (<= a b) (<= b (+ a x)) (= x 0)
                               (or (not (= (f b) (f a))) (and (q a) (not (q (+ b x)))))))))
(set .c2 (and :clauses (.c1) :conclusion ((<= a b))))
(set .c3 (and :clauses (.c1) :conclusion ((<= b (+ a x)))))
(set .c4 (and :clauses (.c1) :conclusion ((= x 0))))
(set .c5 (and :clauses (.c1) :conclusion
           ((or (not (= (f b) (f a))) (and (q a) (not (q (+ b x))))))))
(set .c6 (and_pos :conclusion ((not (and (q a) (not (q (+ b x))))) (q a))))
(set .c7 (and_pos :conclusion ((not (and (q a) (not (q (+ b x))))) (not (q (+ b x))))))
(set .c8 (or :clauses (.c5) :conclusion
           ((not (= (f b) (f a))) (and (q a) (not (q (+ b x)))))))
(set .c9 (eq_congruent :conclusion ((not (= a b)) (= (f b) (f a)))))
(set .c10 (la_disequality :conclusion ((or (= a b) (not (<= a b)) (not (<= b a))))))
(set .c11 (or :clauses (.c10) :conclusion ((= a b) (not (<= a b)) (not (<= b a)))))
(set .c12 (resolution :clauses (.c11 .c2) :conclusion ((= a b) (not (<= b a)))))
(set .c13 (la_generic :conclusion ((not (<= b (+ a x))) (<= b a) (not (= x 0)))))
(set .c14 (resolution :clauses (.c13 .c3 .c4) :conclusion ((<= b a))))
(set .c15 (resolution :clauses (.c12 .c14) :conclusion ((= a b))))
(set .c16 (resolution :clauses (.c9 .c15) :conclusion ((= (f b) (f a)))))
(set .c17 (resolution :clauses (.c8 .c16) :conclusion ((and (q a) (not (q (+ b x)))))))
(set .c18 (resolution :clauses (.c6 .c17) :conclusion ((q a))))
(set .c19 (resolution :clauses (.c7 .c17) :conclusion ((not (q (+ b x))))))
(set .c20 (eq_congruent_pred :conclusion ((not (= a (+ b x))) (not (q a)) (q (+ b x)))))
(set .c21 (resolution :clauses (.c20 .c18 .c19) :conclusion ((not (= a (+ b x))))))
(set .c22 (la_disequality :conclusion ((or (= a (+ b x)) (not (<= a (+ b x))) (not (<= (+ b x) a))))))
(set .c23 (or :clauses (.c22) :conclusion ((= a (+ b x)) (not (<= a (+ b x))) (not (<= (+ b x) a)))))
(set .c24 (resolution :clauses (.c23 .c21) :conclusion ((not (<= a (+ b x))) (not (<= (+ b x) a)))))
(set .c25 (eq_congruent_pred :conclusion
            ((not (= a b)) (not (= (+ a x) (+ b x))) (<= a (+ b x)) (not (<= b (+ a x))))))
(set .c26 (eq_congruent :conclusion ((not (= a b)) (not (= x x)) (= (+ a x) (+ b x)))))
(set .c27 (eq_reflexive :conclusion ((= x x))))
(set .c28 (resolution :clauses (.c26 .c27) :conclusion ((not (= a b)) (= (+ a x) (+ b x)))))
(set .c29 (resolution :clauses (.c25 .c28) :conclusion ((not (= a b)) (<= a (+ b x)) (not (<= b (+ a x))))))
(set .c30 (resolution :clauses (.c29 .c3 .c15) :conclusion ((<= a (+ b x)))))
(set .c31 (resolution :clauses (.c24 .c30) :conclusion ((not (<= (+ b x) a)))))
(set .c32 (la_generic :conclusion ((<= (+ b x) a) (not (= a b)) (not (= x 0)))))
(set .c33 (resolution :clauses (.c32 .c4 .c15 .c31) :conclusion ()))
\end{verbatim}
}
\caption{\label{fig:proofverit} The proof output by veriT for example Formula \ref{eq:example}.  The output has been slightly edited to cut and indent long lines.}
\end{figure}

The first line gives the input.  Clauses {\tt c2} to {c8} explain clausification
of the input.  Equality reasoning produced clauses labeled with {\tt
  eq\_congruent(\_pred)} and {\tt eq\_reflexive}, whereas the linear arithmetic
module produced all the clauses labeled {\tt la\_disequality} and {\tt
  la\_generic}.  The rest of the proof is mainly resolution applications, and ends with the empty clause.

% PF I should maybe say more...

In the future, the proof format of veriT will be further improved to even better
stick to the SMT-LIB standard.  In particular, when using shared terms (for
simplicity, the given proof example does not use shared terms), veriT uses a
notation to label repeating formulas which is inspired from early CVC custom
input language, but which does not fit well with the SMT-LIB language.

\section{The Z3 SMT Solver}
\label{sec:z3}

Z3 is a Satisfiability Modulo Theories (SMT) solver developed at
Microsoft Research.  Z3 source code is available online, and it is
freely for non-commercial purposes.  Z3 is used in various software
analysis and test-case generation projects at Microsoft Research and
elsewhere.  Proof generation is based on two simple ideas: (1) a
notion of implicit quotation to avoid introducing auxiliary variables,
it simplifies the creation of proof objects considerably; (2) natural
deduction style proofs to facilitate modular proof re-construction.

%\subsection{Proof Format and Production}

In Z3, proof objects are represented as terms. So a proof-tree is just a term
where each inference rule is represented by a function symbol.
For example, consider the proof-rule $mp(p, q, \varphi)$, where $p$ is a proof for
$\psi \rightarrow \varphi$ and $q$ is a proof for $\psi$. Each proof-rule has
\emph{consequent}, the consequent of $mp(p, q, \varphi)$ is $\varphi$.

A basic underlying principle for composing and building proofs in Z3 has been to support a modular
architecture that works well with theory solvers that receive literal assignments from other solvers and
produce contradictions or new literal assignments. The theory solvers should be able to produce independent
and opaque explanations for their decisions.
Conceptually, each solver acts upon a set of hypotheses and produce a consequent. The basic proof-rules
that support such an architecture can be summarized as: \emph{hypothesis} , that allow introducing an
assumption, \emph{lemma}, that eliminates hypotheses, and \emph{unit resolution} that handles basic propagation.
We say that a proof-term is \emph{closed} when every path that ends with a hypothesis contains an application
of rule lemma. If a proof-term is not closed, it is open.

The main propositional inference engine in Z3 is based on a DPLL(T) architecture.
The DPLL(T) proof search method lends itself naturally to producing resolution style proofs.
Systems, such as zChaff, and a version of MiniSAT, produce proof logs based on logging the
unit propagation steps as well as the conflict resolution steps. The resulting log suffices to produce a
propositional resolution proof.

The approach taken in Z3 bypasses logging, and instead builds proof
objects during conflict resolution.  With each clause we attach a
proof. Clauses that were produced as part of the input have proofs
that were produced from the previous steps. This approach does not
require logging resolution steps for every unit-propagation, but
delays the analysis of which unit propagation steps are useful until
conflict resolution. The approach also does not produce a resolution
proof directly. It produces a natural deduction style proof with
hypotheses.

The theory of equality can be captured by axioms for reflexivity,
symmetry, transitivity, and substitutivity of equality. In Z3,
these axioms are inference rules, and these inference rules apply
for any binary relation that is reflexive,
symmetric, transitive, and/or reflexive-monotone.

In the DPLL(T) architecture, decision procedures for a theory $T$
identify sets of asserted $T$-inconsistent literals. Dually, the
disjunction of the negated literals are $T$-tautologies. Consequently,
proof terms created by theories can be summarized using a single form,
here called \emph{theory lemmas}. Some theory lemmas are annotated
with hints to help proof checkers and reconstruction. For example, the
theory of linear arithmetic produces theory lemmas based on Farka's
lemma. For example, suppose $p$ is a proof for $x > 0$, and $q$ is a
proof for $2x + 1 < 0$, then $\mbox{\emph{farkas}}(1, p, -1/2, q, \neg(x > 0) \vee \neg(2x + 1 < 0))$
is a theory lemma where the coefficients $1$ and $-1/2$ are hints.



The Z3 simplifier applies standard simplification rules for the
supported theories. For example, terms using the arithmetical operations,
both for integer, real, and bit-vector arithmetic, are normalized into sums of monomials.
A single proof rule called \emph{rewrite} is used to record the simplification steps.
For example, $\mbox{\emph{rewrite}}(x + x = 2 x)$ is a proof for $x + x = 2x$.

Notice that Z3 does not axiomatize the legal rewrites.
Instead, to check the rewrite steps, one must rely on a
proof checker to be able to apply similar inferences for the set of built-in theories.
Thus, like VeriT, Z3 proofs are coarse grained and rather falls into the “proof trace”
category rather than full detailed proof.

Since Z3 proofs are terms, they can be traversed using the Z3 API.
Proofs can also be outputed in SMT-LIB 2.0 standard. A sample proof
for our running example~\ref{eq:example} is given in
Figure~\ref{fig:proofz3}.

\begin{figure}
{\scriptsize
\begin{verbatim}
(let (($x82 (q b)) (?x49 (* (- 1.0) b)) (?x50 (+ a ?x49))
      ($x51 (<= ?x50 0.0)) (?x35 (f b)) (?x34 (f a))
      ($x36 (= ?x34 ?x35)) ($x37 (not $x36))
      ($x43 (or $x37 (and (q a) (not (q (+ b x))))))
      ($x33 (= x 0.0)) (?x57 (+ a ?x49 x)) ($x56 (>= ?x57 0.0))
      ($x44 (and (<= a b) (<= b (+ a x)) $x33 $x43))
      (@x60 (monotonicity (rewrite (= (<= a b) $x51))
                          (rewrite (= (<= b (+ a x)) $x56))
                          (= $x44 (and $x51 $x56 $x33 $x43))))
      (@x61 (mp (asserted $x44) @x60 (and $x51 $x56 $x33 $x43)))
      (@x62 (and-elim @x61 $x51)) ($x71 (>= ?x50 0.0)))
(let ((@x70 (trans (monotonicity (and-elim @x61 $x33) (= ?x57 (+ a ?x49 0.0)))
                   (rewrite (= (+ a ?x49 0.0) ?x50)) (= ?x57 ?x50))))
(let ((@x74 (mp (and-elim @x61 $x56) (monotonicity @x70 (= $x56 $x71)) $x71)))
(let ((@x121 (monotonicity (symm ((_ th-lemma arith eq-propagate 1 1) @x74 @x62 (= a b)) (= b a))
                           (= $x82 (q a)))))
(let (($x38 (q a)) ($x96 (or (not $x38) $x82)) ($x97 (not $x96)))
(let ((@x115 (monotonicity (symm ((_ th-lemma arith eq-propagate 1 1) @x74 @x62 (= a b)) (= b a))
                           (= ?x35 ?x34))))
(let (($x100 (or $x37 $x97)))
(let ((@x102 (monotonicity (rewrite (= (and $x38 (not $x82)) $x97))
                           (= (or $x37 (and $x38 (not $x82))) $x100))))
(let (($x85 (not $x82)))
(let (($x88 (and $x38 $x85)))
(let (($x91 (or $x37 $x88)))
(let ((@x81 (trans (monotonicity (and-elim @x61 $x33) (= (+ b x) (+ b 0.0)))
                   (rewrite (= (+ b 0.0) b)) (= (+ b x) b))))
(let ((@x87 (monotonicity (monotonicity @x81 (= (q (+ b x)) $x82)) (= (not (q (+ b x))) $x85))))
(let ((@x93 (monotonicity (monotonicity @x87 (= (and $x38 (not (q (+ b x)))) $x88)) (= $x43 $x91))))
(let ((@x103 (mp (mp (and-elim @x61 $x43) @x93 $x91) @x102 $x100)))
(let ((@x119 (unit-resolution (def-axiom (or $x96 $x38))
                              (unit-resolution @x103 (symm @x115 $x36) $x97) $x38)))
(let ((@x118 (unit-resolution (def-axiom (or $x96 $x85))
                              (unit-resolution @x103 (symm @x115 $x36) $x97) $x85)))
(unit-resolution @x118 (mp @x119 (symm @x121 (= $x38 $x82)) $x82) false)))))))))))))))))
\end{verbatim}
}
\caption{\label{fig:proofz3} The proof output by Z3 for example Formula \ref{eq:example}. The output has been slightly edited to cut and indent long lines.}
\end{figure}



\section{The Lean Prover}
\label{sec:lean}
Lean is a new open source theorem prover being developed by Leonardo
de Moura and Soonho Kong~\cite{lean}. Lean is not a standard SMT
solver, it can be used as an automatic prover like SMT solvers, but it
can also be used as a proof assistant. Lean kernel is based on
dependent type theory, and is implemented in two layers. The first
layer contains the type checker, and APIs for creating and
manipulating the terms, declarations and the environment. The first
layer has several configuration options. For example, developers may
instantiate the kernel with or without an impredicative Prop sort.
They may also select whether Prop is proof irrelevant or not. The
first layer consists of 5k lines of C++ code. The second layer
provides additional components such as inductive families (500
additional lines of code). When the kernel is instantiated, one
selects which of these components should be used. The current
components are already sufficient for producing an implementation of
the Calculus of Inductive Constructions (CIC). The main difference is
that Lean does not have universe cumulativity, it instead provides
universe polymorphism. The Lean CIC-based kernel is treated as the standard
kernel. Another design goal is to support the new Homotopy Type System
(HTS) proposed by Vladimir Voevodsky. HTS is going to be implemented
as another kernel instantiation.

Lean is meant to be used as a standalone system and as a software
library. It provides an extensive API and can be easily embedded in
other systems. SMT solvers can use the Lean API to create proof terms
that can be independently checked. The API can also be used to export
Lean proofs to other systems based on CIC (e.g., Coq and Matita).

Having a more expressive language for encoding proofs provides several
advantages. First, we can easily add new ``proof rules'' without
modifying the proof checker (i.e., type checker). Proof rules such as
\emph{mp} and \emph{monotonicity} used in Z3 are just theorems in Lean.
When a new decision procedure (or any other form of
automation) is implemented, the developer must first prove the theorems
that are needed to justify the results produced by the automatic
procedure. For example, suppose a developer is implementing a procedure for
Presburger arithmetic, she will probably use a theorem such as:

{\scriptsize
\begin{verbatim}
theorem add_comm (n m:nat) : n + m = m + n
:=
  induction_on m
    (trans (add_zero_right _) (symm (add_zero_left _)))
    (take k IH,
      calc
        n + succ k = succ (n+k) : add_succ_right _ _
          ... = succ (k + n) : {IH}
          ... = succ k + n : symm (add_succ_left _ _))
\end{verbatim}
}

Pre-processing steps such as skolemization can be supported in
a similar way. Whenever a preprocessing procedure applies a
skolemization step, it uses the following theorem to justify it.

{\scriptsize
\begin{verbatim}
theorem skolem_th {A : Type} {B : A -> Type} {P : forall x : A, B x -> Bool} :
        (forall x, exists y, P x y) = (exists f, (forall x, P x (f x)))
:= iff_intro
      (assume H : (forall x, exists y, P x y), @axiom_of_choice _ _ P H)
      (assume H : (exists f, (forall x, P x (f x))),
             take x, obtain (fw : forall x, B x) (Hw : forall x, P x (fw x)), from H,
                  exists_intro (fw x) (Hw x))
\end{verbatim}
}

Most SMT solvers make extensive use of preprocessing steps, and as
pointed out before, it is not easy to be proof producing for every
single rewriting step that can occur in the solver.
In Lean, this issue is addressed by providing a generic rewriting
engine that can use any previously proved theorems. The engine
accepts two kinds of theorems: congruence theorems and (conditional)
equations. For example, the following two theorems can be used to
distribute universal quantifiers over disjuctions when the left
(right) hand side does not reference the bound variable.

{\scriptsize
\begin{verbatim}
theorem forall_or_distributel {A : Type} (p : Bool) (q : A -> Bool)
           : (forall x, q x \/ p) = ((forall x, q x) \/ p)
theorem forall_or_distributer {A : Type} (p : Bool) (q : A -> Bool)
           : (forall x, p \/ q x) = (p ∨ forall x, q x)
\end{verbatim}
}

%\section{Proof Formats}

% Briefly describe the actual text formats used to output proofs.

% Are there standard formats in your community? If not, why not?

% If possible and if not too large, please copy-paste the
% grammar of the proof format(s) here.

% Please copy-paste a small but interesting example proof here.
% Try to select an example that shows (most of) the peculiarities
% of your proof format.
% Use a verbatim environment (like the listings package, for example).

% What are the guiding principles involved in the design of the format?
% Is it intended to be human-readable? easy to parse?
% easy/efficient to verify? as small as possible?
% In case of automated proofs, is it intended to recover
% intuition of the proofs?

% If a proof expressed in your proof format is intended
% to be checked by another tool, what is the distribution
% between the information that is stored in the
% proof and the information that has to be reconstructed by
% the tool verifying the proof.

% Is there any relation between the proof format and
% the format used for input problems?

% How fine- or coarse-grained are the proofs in this format?
% Are you and your community satisfied with this level of detail?
% Are there applications that might benefit from
% a greater level of detail?

% If you provide several proof formats,
% explain the kinds of users that are targeted by these formats.

% Are there generic parsers available for this format?
% In which programming languages?

% What are the trends?


%\section{Proof Production}

% What is the ``price'' (e.g. in terms of efficiency or memory overhead)
% paid to generate proofs? How much slower does the tool become?
% How much more memory does it consume?

% In case of automated proofs, how different is the proof-producing
% proof search algorithm from the proof search algorithm that simply
% answers "yes" or "no"?

% Is it usually possible to switch proof production
% on and off in your kind of tool?

% How is the code for proof production?
% Does it require substantial intervention in the usual architecture
% and data structures of a non-proof-producing tool of the same kind?
% Could you show some code snippets here, in order to
% illustrate proof production?
% If there is a significant gap between the proof system and
% the actual data structures used by the tool,
% could you illustrate (perhaps with a simple example)
% how a proof can be obtained from these data structures?

% Does your kind of tool keep the (partial) proof in memory during
% the search and writes it to a file only when the search finishes?
% Or does it write partial results (lemmas) eagerly to file?
% In this latter case, are there techniques to cope with
% excessive generation and writing of lemmas that may not be
% relevant to prove the theorem?

% What are the trends?


%\section{Proof Applications}

% Which application domains have used your kind of tool?
% Have there been any ground-breaking achievements?
% Among these application domains,
% which are particularly interested in the generated proofs?
% How do they use the proofs? Do they use proofs just for
% certifying the correctness of the provided answer?
% Or do they extract other kinds of information
% (e.g. unsat cores, interpolants, witnesses, programs, ...) from proofs?

% In the context of these applications, when should
% a proof P of a certain theorem be considered better than
% a proof Q of the same theorem?
% For example, in case of double negation transformations,
% why is it better to get intuitionistic proofs from classical proofs?

% What are the trends?

\section{Applications}
\label{sec:app}

\subsection{veriT}

Users targeted by our format are people who would like to replay those proofs into proof assistants, people working on interpolation (veriT does not provide a service for interpolation), and people wanting unsat cores (proofs are here overkill, but proofs are a way to get unsat cores without getting into the solver and design special techniques).

Georg Hofferek has used our proofs for multiple interpolant computation in some synthesis problems (TODO more details/citation later).  In the Rodin plugin to SMT, veriT's proofs are used as a mean to quickly extract a core.  Proofs from veriT are used in Coq-SMT (TODO should check) for proof reconstruction of SMT proofs.

\subsection{Z3}

The two main applications for Z3 proof certificates are: proof
reconstruction in interactive proof assistant such as
Isabelle~\cite{IsabelleZ3}; and interpolation generation~\cite{iZ3}.

In Isabelle/HOL, Z3 proofs are reconstructed in a completely different
system based on a secure proof kernel. Proof reconstruction is a quite
involved process, because proof rules such as \emph{theory lemmas} require
several steps of reasoning in Isabelle/HOL.

The interpolation prover iZ3~\cite{iZ3} is implemented on top of Z3.  It uses Z3
proof as guide for construction of a proof by a secondary, less
efficient, interpolating prover. It translates Z3 proofs into
a proof calculus that does admit feasible interpolation, with
``gaps'', or lemmas, that must be discharged by the secondary prover.

\section{Conclusions}
\label{sec:concl}

% Summarize the most important points of the previous sections.

Building a proof-producing SMT solver is mostly easy: the underlying SAT solver
generate resolution proofs, and, as long as the theories in the combination also
produce certificates, these can be combined easily simply using again
resolution.  There are however many small and larger challenges related to SMT
proofs.  First, it is necessary to collect and store all the necessary
information to produce the final proof; this is mostly a technical problem, but,
this can be the bottleneck of the proof search.  Indeed this may involve an
enormous amount of bookkeeping work, and, if using main memory, it may quickly
exhaust the available memory.  Second, SMT often relies on many preprocessing
techniques, some being necessary for the soundness of tool, some being useful
for efficiency.  The attitude followed by most SMT developer is to provide some
kind of high level trace for essential preprocessing.  Non-essential
preprocessing is turned off, with an important consequence on efficiency.  It
may also be non-trivial to generate good proofs for some preprocessing
techniques, e.g.\ for Skolemization, or symmetry breaking.  And finally, SMT
solvers may use external tools as specialized reasoners for some
theories.\marginpar{do we need citations? GLPK-CVC, harvey-E} Producing a proof
in SMT may thus, in some cases, require certificates from those external
reasoners.

An important challenge for the SMT community is to provide a proof format which
is sufficiently flexible to accommodate all needs, sufficiently compact to be
practical, and sufficiently elegant (which is very subjective) to be accepted by
most.  Some formats have been proposed~\cite{Besson1,dMB08,Reynolds1}, but it
seems it is still too early for a consensus on how to represent SMT proofs.

% Thank you very much for your contribution!

\medskip\noindent%
\emph{Acknowledgements}: Pascal Fontaine would like to thank David D\'{e}harbe,
who jointly designed veriT with him.


\bibliographystyle{plain}
\bibliography{SMTProofs}

\end{document}
